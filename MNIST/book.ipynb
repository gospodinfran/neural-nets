{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100.0%\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training = datasets.MNIST(\"data\", train=True,  download=True, transform=ToTensor())\n",
    "test = datasets.MNIST(\"data\", train=False,  download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training, batch_size=64)\n",
    "test_dataloader = DataLoader(test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_train_dataloader = DataLoader(training, batch_size=64)\n",
    "cnn_test_dataloader = DataLoader(test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, label in cnn_train_dataloader:\n",
    "    images = images.view(-1, 1, 28, 28)\n",
    "for images, label in cnn_test_dataloader:\n",
    "    images = images.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 300, device=device),\n",
    "            nn.BatchNorm1d(300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 100),\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conv_net = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc_net = nn.Sequential(\n",
    "            nn.Linear(7*7*64, 300),\n",
    "            nn.BatchNorm1d(300),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(300, 100),\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(100, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_net(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNet()\n",
    "model.load_state_dict(torch.load(\"MNIST_weights.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn = CNN()\n",
    "model_cnn.load_state_dict(torch.load(\"MNIST_cnn_weights.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "optimizer_adam = torch.optim.Adam(model.parameters(), lr=1.8e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn_cnn = nn.CrossEntropyLoss()\n",
    "optimizer_cnn = torch.optim.Adam(model_cnn.parameters(), lr=2.52e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    for batch, (X, Y) in enumerate(dataloader):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, Y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f\"Loss: {loss.item()}, batch: {batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn, iteration):\n",
    "    num_batches = len(dataloader)\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (X, Y) in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, Y)\n",
    "            correct += (pred.argmax(1) == Y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "\n",
    "    print(f\"Test {iteration+1} Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.5611479941289872e-05, batch: 0\n",
      "Loss: 0.0004736580594908446, batch: 100\n",
      "Loss: 0.00038856820901855826, batch: 200\n",
      "Loss: 0.00012981124746147543, batch: 300\n",
      "Loss: 0.003494004253298044, batch: 400\n",
      "Loss: 0.011710887774825096, batch: 500\n",
      "Loss: 0.00791223719716072, batch: 600\n",
      "Loss: 0.00012096855789422989, batch: 700\n",
      "Loss: 0.00045427068835124373, batch: 800\n",
      "Loss: 0.0005726410308852792, batch: 900\n",
      "Test 1 Error: \n",
      " Accuracy: 97.1%, Avg loss: 0.192404 \n",
      "\n",
      "Loss: 0.0001256070245290175, batch: 0\n",
      "Loss: 0.0010707556502893567, batch: 100\n",
      "Loss: 0.02220996469259262, batch: 200\n",
      "Loss: 6.374660006258637e-05, batch: 300\n",
      "Loss: 3.0176544896676205e-05, batch: 400\n",
      "Loss: 0.0015852610813453794, batch: 500\n",
      "Loss: 9.841487553785555e-06, batch: 600\n",
      "Loss: 0.01866433583199978, batch: 700\n",
      "Loss: 0.0006178961484692991, batch: 800\n",
      "Loss: 0.0001836982264649123, batch: 900\n",
      "Test 2 Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.146452 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# epochs = 2\n",
    "# for t in range(epochs):\n",
    "#     train_loop(train_dataloader, model, loss_fn, optimizer_adam)\n",
    "#     test_loop(test_dataloader, model, loss_fn, t)\n",
    "# torch.save(model.state_dict(), \"MNIST_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.005144428927451372, batch: 0\n",
      "Loss: 3.269657463533804e-05, batch: 100\n",
      "Loss: 0.0009662464144639671, batch: 200\n",
      "Loss: 0.0006048443028703332, batch: 300\n",
      "Loss: 0.06644224375486374, batch: 400\n",
      "Loss: 0.0011701879557222128, batch: 500\n",
      "Loss: 0.001153750461526215, batch: 600\n",
      "Loss: 0.031159499660134315, batch: 700\n",
      "Loss: 0.012692232616245747, batch: 800\n",
      "Loss: 0.010461484082043171, batch: 900\n",
      "Test 1 Error: \n",
      " Accuracy: 98.7%, Avg loss: 0.074482 \n",
      "\n",
      "Loss: 0.042608119547367096, batch: 0\n",
      "Loss: 0.0007200156687758863, batch: 100\n",
      "Loss: 0.005838931538164616, batch: 200\n",
      "Loss: 0.04209429398179054, batch: 300\n",
      "Loss: 0.0006708378787152469, batch: 400\n",
      "Loss: 0.0003395329404156655, batch: 500\n",
      "Loss: 0.0026480327360332012, batch: 600\n",
      "Loss: 0.004262932576239109, batch: 700\n",
      "Loss: 0.04197099432349205, batch: 800\n",
      "Loss: 0.10210076719522476, batch: 900\n",
      "Test 2 Error: \n",
      " Accuracy: 98.9%, Avg loss: 0.067956 \n",
      "\n",
      "Loss: 0.00013056941679678857, batch: 0\n",
      "Loss: 0.001052644569426775, batch: 100\n",
      "Loss: 0.002975482027977705, batch: 200\n",
      "Loss: 0.0023453221656382084, batch: 300\n",
      "Loss: 0.000417105620726943, batch: 400\n",
      "Loss: 0.025698883458971977, batch: 500\n",
      "Loss: 6.062169995857403e-05, batch: 600\n",
      "Loss: 0.001934484695084393, batch: 700\n",
      "Loss: 0.0007136307540349662, batch: 800\n",
      "Loss: 0.003237488679587841, batch: 900\n",
      "Test 3 Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.055583 \n",
      "\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "epochs2 = 3\n",
    "for t in range(epochs2):\n",
    "    train_loop(cnn_train_dataloader, model_cnn, loss_fn_cnn, optimizer_cnn)\n",
    "    test_loop(cnn_test_dataloader, model_cnn, loss_fn_cnn, t)\n",
    "print(\"DONE\")\n",
    "torch.save(model_cnn.state_dict(), \"MNIST_cnn_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
